{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3e81da",
   "metadata": {},
   "source": [
    "Дополним датасет лемматизированным текстом комментариев. Для этого сохраним леммы частых токенов и потом обработаем все тексты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fa4028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import gensim\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import ufal.udpipe\n",
    "from model import Model\n",
    "import conllu\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from nltk.parse import DependencyGraph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.text import Text\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import plotly.express as px\n",
    "import emoji\n",
    "\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd45fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3062ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmes = {}\n",
    "for key in tqdm(dict(Counter(tokens).most_common(100000))):\n",
    "    lemmes[key] = morph.parse(key)[0].normal_form\n",
    "\n",
    "    \n",
    "def lemmed_sent(sent):\n",
    "    new_sent = []\n",
    "    for p in punctuation:\n",
    "        sent = sent.replace(p, '')\n",
    "    for word in sent.split():\n",
    "        razbor = ''\n",
    "        if word in lemmes.keys():\n",
    "            razbor = lemmes[word]\n",
    "        else:\n",
    "            razbor = morph.parse(word)[0].normal_form\n",
    "        new_sent.append(razbor)\n",
    "    return ' '.join(new_sent)\n",
    "\n",
    "data['lemmed_comment'] = data['text'].progress_apply(lemmed_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59218858",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('new_final_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012e5c99",
   "metadata": {},
   "source": [
    "Соберём список интенсификаторов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3bcaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_model = Model('russian-taiga-ud-2.4-190531.udpipe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3ce5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = '\\n'.join(str(a) for a in list(data['text']))\n",
    "with open('comms.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(reviews)\n",
    "    \n",
    "data = gensim.models.word2vec.LineSentence('comms.txt')\n",
    "%time model_rev = gensim.models.Word2Vec(data, vector_size=500, window=5, min_count=5, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b7e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_more_words(list_start):\n",
    "    final_namings = []\n",
    "    final_namings.extend(list_start)\n",
    "    for name in list_start:\n",
    "        for (word, cos) in model_rev.wv.most_similar(name):\n",
    "            if cos >= 0.5:\n",
    "                if word[-1] in punctuation:\n",
    "                    final_namings.append(word[:-1])\n",
    "                else:\n",
    "                    final_namings.append(word)\n",
    "    final_namings = [a.lower() for a in final_namings]\n",
    "    final_namings = list(set(final_namings))\n",
    "    return final_namings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3300506",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_namings = ['очень', 'крайне']\n",
    "final = get_more_words(my_namings)\n",
    "\n",
    "final_2 = get_more_words(final)\n",
    "\n",
    "final_3 = get_more_words(final_2)\n",
    "\n",
    "real_final = ['абсолютно', 'адски', 'безгранично', 'безмерно', 'безумно', \n",
    "              'бесконечно', 'беспредельно', 'более-менее', 'весьма', 'всецело', \n",
    "              'давольно', 'дико', 'действительно', 'довольно', 'достаточно', \n",
    "              'дюже', 'едва', 'еле', 'исключительно', 'катастрофически', \n",
    "              'колоссально', 'крайне',  'критически', 'максимально', 'наиболее', \n",
    "              'наименее', 'настолько', 'невероятно', 'недостаточно', 'невыразимо', \n",
    "              'неимоверно', 'немного', 'непередаваемо', 'несказанно', 'несколько', \n",
    "              'отвратительно', 'отчасти', 'оч', 'очень', 'очень-очень', 'оочень', \n",
    "              'ооочень', 'ооооочень', 'полностью', 'поразительно', 'почти', \n",
    "              'практически', 'реально', 'сильно', 'сказочно', 'слегка', 'слишком', \n",
    "              'смертельно', 'совершенно', 'совсем', 'столь', 'страшно', 'удивительно', \n",
    "              'ужасно', 'чертовски', 'чрезвычайно', 'чудовищно', 'чуть-чуть',  \n",
    "              'фантастически', 'феноменально']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7bebbd",
   "metadata": {},
   "source": [
    "Подготовим синтаксический разбор и функцию, которая будет находить что-то, похожее на наречные интенсификаторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7157caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conllu(model, te):\n",
    "    sentences = model.tokenize(te)\n",
    "    for s in sentences:\n",
    "        model.tag(s)\n",
    "        model.parse(s)\n",
    "    conllu_text = model.write(sentences, \"conllu\")\n",
    "    trees = []\n",
    "    for sent in conllu_text.split('\\n\\n'):\n",
    "        tree = [line for line in sent.split('\\n') if line and line[0] != '#']\n",
    "        trees.append('\\n'.join(tree))\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e8e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_adv_intensifiers(t):\n",
    "    intense_cols = []\n",
    "    trees = get_conllu(ud_model, t)\n",
    "    for tr in trees:\n",
    "        if tr.endswith('\\s'):\n",
    "            tr = tr[:-3]\n",
    "        new_tr = tr.replace('SpacesAfter= ', 'SpacesAfter=')\n",
    "        new_tr = new_tr.replace('SpacesAfter=\\t', 'SpacesAfter=')\n",
    "        new_tr = new_tr.replace('SpaceAfter= ', 'SpaceAfter=')\n",
    "        new_tr = new_tr.replace('SpaceAfter=\\t', 'SpaceAfter=')\n",
    "        try:\n",
    "            d = DependencyGraph(new_tr)\n",
    "            d.root = d.nodes[0]\n",
    "            trips = list(d.triples())\n",
    "            for triple in trips:\n",
    "                if triple[2][0].lower() in real_final and triple[1] in ['advmod', 'obl']:\n",
    "                    intense_cols.append(triple[2][0] + ' ' + triple[0][0])\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "    return intense_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539b3841",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['intensified'] = data['text'].progress_apply(find_adv_intensifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff70a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['intensified'] != \"[]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb335d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_data = intense_data['intensified'].to_list()\n",
    "new_int_data = []\n",
    "for a in fake_data:\n",
    "    a = a.replace('[', '')\n",
    "    a = a.replace(']', '')\n",
    "    a = a.replace(\"'\", '')\n",
    "    a = a.replace(\"\\\"\", '')\n",
    "    a = a.split(', ')\n",
    "    new_int_data.extend(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1413db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "intense_data.loc[intense_data['intensified']== '[]', 'intensified'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b1c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "intensifiers_table = []\n",
    "for comment in parsed:\n",
    "    if len(comment['intensified']) > 2:\n",
    "        a = comment['intensified'].replace('[', '')\n",
    "        a = a.replace('[', '')\n",
    "        a = a.replace(']', '')\n",
    "        a = a.replace(\"'\", '')\n",
    "        a = a.replace(\"\\\"\", '')\n",
    "        ints = a.split(', ')\n",
    "        if len(ints) == 1:\n",
    "            one = {}\n",
    "            one['intensified'] = ints[0]\n",
    "            one['text'] = comment['text']\n",
    "            one['from_id'] = comment['from_id'] \n",
    "            one['group'] = comment['group']\n",
    "            one['datetime'] = comment['datetime']\n",
    "            one['name'] = comment['name']\n",
    "            one['bdate'] = comment['bdate']\n",
    "            one['city'] = comment['city']\n",
    "            one['sex'] = comment['sex']\n",
    "            intensifiers_table.append(one)\n",
    "        else:\n",
    "            for i in ints:\n",
    "                one = {}\n",
    "                one['intensified'] = i\n",
    "                one['text'] = comment['text']\n",
    "                one['from_id'] = comment['from_id'] \n",
    "                one['group'] = comment['group']\n",
    "                one['datetime'] = comment['datetime']\n",
    "                one['name'] = comment['name']\n",
    "                one['bdate'] = comment['bdate']\n",
    "                one['city'] = comment['city']\n",
    "                one['sex'] = comment['sex']\n",
    "                intensifiers_table.append(one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd38ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "intense_df = pd.DataFrame(intensifiers_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d39c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pmi(intensifier, word, freq, df):\n",
    "    p_very = len(df)/len(lemmed_corp)\n",
    "    p_w = sum([1 for a in lemmed_corp if a.lower() == word])/len(lemmed_corp)\n",
    "    p_very_w = freq/len(lemmed_corp)\n",
    "    if p_w*p_very > 0:\n",
    "        spl = p_very_w/(p_w*p_very)\n",
    "        return math.log2(spl)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_pos(all_pos):\n",
    "    values = values = ['PRCL', 'NUMR']\n",
    "    all_pos = [v for v in all_pos if v not in values]\n",
    "    all_pos = ['глагол' if x in ['VERB', 'INFN', 'GRND', 'PRTF', 'PRTS'] else x for x in all_pos]\n",
    "    all_pos = ['наречие' if x in ['ADVB'] else x for x in all_pos]\n",
    "    all_pos = ['прилагательное' if x in ['ADJF', 'ADJS', 'COMP'] else x for x in all_pos]\n",
    "    all_pos = ['существительное' if x in ['NOUN'] else x for x in all_pos]\n",
    "    all_pos = ['предикат' if x in ['PRED'] else x for x in all_pos]\n",
    "    all_pos = ['междометие' if x in ['INTJ'] else x for x in all_pos]\n",
    "    all_pos = ['местоимение' if x in ['NPRO'] else x for x in all_pos]\n",
    "    all_pos = ['эмоджи' if x in ['EMOJ'] else x for x in all_pos]\n",
    "    p = dict(Counter(all_pos))\n",
    "    return '\\n'.join(': '.join(str(e) for e in a) for a in list({k: v for k, v in sorted(p.items(), key=lambda item: item[1])}.items())[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87261f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_word_stat(word):\n",
    "    print(f'Статистика для слова {word}')\n",
    "    word_df = intense_df[intense_df['intensifier'] == word]\n",
    "    lentach_percent = round(len(word_df[word_df['group'] == 'Лентач'])/len(word_df)*100, 2)\n",
    "    palata_percent = round(len(word_df[word_df['group'] == 'Палата №6'])/len(word_df)*100, 2)\n",
    "    ria_percent = round(len(word_df[word_df['group'] == 'РИАНовости'])/len(word_df)*100, 2)\n",
    "    print(f'Соотношение в группах Лентач/РиаНовости/Палата №6: {lentach_percent}/{palata_percent}/{ria_percent}')\n",
    "    ipm_fem = len(word_df[word_df['sex'] == 1])/female_total_words*1000000\n",
    "    ipm_male = len(word_df[word_df['sex'] == 2])/male_total_words*1000000\n",
    "    print(f'IPM для слова {word} для женщин/мужчин: {round(ipm_fem, 2)}/{round(ipm_male, 2)}')\n",
    "    if ipm_male > 0 and ipm_fem > 0:\n",
    "        if ipm_fem > ipm_male:\n",
    "            print(f'Женщины употребляют слово {word} чаще в {round(ipm_fem/ipm_male, 2)} раз(а)')\n",
    "        else:\n",
    "            print(f'Мужчины употребляют слово {word} чаще в {round(ipm_male/ipm_fem, 2)} раз(а)')\n",
    "    print(f'Распределение по частям речи: {change_pos(word_df[\"pos\"].to_list())}')\n",
    "    vv = dict(Counter(word_df['normed'].to_list()))\n",
    "    pmis = {}\n",
    "    vv = {k: v for k, v in vv.items() if v > 1}\n",
    "    print(vv)\n",
    "    for second in vv.keys():\n",
    "        print(f'PMI для слов {word} + {second}')\n",
    "        pmis[second] = round(calculate_pmi(word, second, len(word_df[word_df['normed'] == second]), word_df), 3)\n",
    "    print(list({k: v for k, v in sorted(pmis.items(), key=lambda item: item[1])}.items())[::-1][:10])\n",
    "    print(sorted(dict(Counter([a for a in word_df['age_group'] if a != None])).items()))\n",
    "    print('_______________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace4ab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year(st):\n",
    "    if len(st.split('.')) == 3:\n",
    "        year = st.split('.')[2]\n",
    "        if int(year) > 1940:\n",
    "            return year\n",
    "    return 0\n",
    "\n",
    "def get_age(birth, dt):\n",
    "    return int(dt.split()[0].split('.')[2]) - int(birth)\n",
    "\n",
    "def age_group(age):\n",
    "    if 14 <= age <= 24:\n",
    "        return '14-24'\n",
    "    elif 25 <= age <= 30:\n",
    "        return '25-30'\n",
    "    elif 31 <= age <= 36:\n",
    "        return '31-36'\n",
    "    elif 37 <= age <= 45:\n",
    "        return '37-45'\n",
    "    elif 46 <= age <= 90:\n",
    "        return '46+'\n",
    "\n",
    "intense_df['birthyear'] = intense_df['bdate'].apply(get_year)\n",
    "\n",
    "intense_df['age'] = intense_df.apply(lambda x: get_age(x.birthyear, x.datetime), axis=1)\n",
    "\n",
    "intense_df['age_group'] = intense_df['age'].apply(age_group)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
